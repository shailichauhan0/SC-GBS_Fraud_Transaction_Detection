# -*- coding: utf-8 -*-
"""Bank Transaction Fraud Detection Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hX5RH8tlzusiC_YLDNppkzHVY2jghCGM

# **Importing Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
# %matplotlib inline

# Setting display options
pd.set_option('display.max_columns', None)

"""# **Loading and Overview of the Dataset**"""

# Loading the dataset
dataset_path = '/content/bank_transactions_data_2.csv'
df = pd.read_csv(dataset_path)

# Displaying the first five rows of the dataset
print("First five rows of the dataset:")
display(df.head())

# Getting basic information about the dataset
print("\nDataset information:")
print(df.info())

# Checking for missing values
print("\nMissing values in each column:")
print(df.isnull().sum())

# Statistical summary of numerical columns
print("\nStatistical summary of numerical columns:")
display(df.describe())

# Checking for duplicate entries
print("\nNumber of duplicate rows:")
print(df.duplicated().sum())

from google.colab import drive
drive.mount('/content/drive')

"""# **Data Preprocessing**"""

# Converting date columns to datetime objects
df['TransactionDate'] = pd.to_datetime(df['TransactionDate'])
df['PreviousTransactionDate'] = pd.to_datetime(df['PreviousTransactionDate'])

# Extracting new features from date columns
df['TransactionHour'] = df['TransactionDate'].dt.hour
df['TransactionDay'] = df['TransactionDate'].dt.dayofweek  # Monday=0, Sunday=6

# Handling missing values (if any)
# Since there are no missing values, we proceed without imputations

# Checking for negative account balances
if (df['AccountBalance'] < 0).any():
    print("\nTransactions with negative account balance found.")
else:
    print("\nNo transactions with negative account balance.")

"""# **Exploratory Data Analysis (EDA)**

## **Plotting distributions of numerical variables**
"""

# Univariate Analysis: Plotting distributions of numerical variables
numerical_cols = ['TransactionAmount', 'AccountBalance', 'CustomerAge', 'TransactionDuration', 'LoginAttempts']
plt.figure(figsize=(15, 8))
for i, col in enumerate(numerical_cols):
    plt.subplot(2, 3, i+1)
    sns.histplot(df[col], kde=True, bins=30)
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

"""## **Categorical Variables Analysis**"""

# Categorical Variables Analysis
categorical_cols = ['TransactionType', 'Location', 'Channel', 'CustomerOccupation']
for col in categorical_cols:
    plt.figure(figsize=(12, 6))
    sns.countplot(x=col, data=df, order=df[col].value_counts().index)
    plt.title(f'Count Plot of {col}')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.show()

"""## **Correlation Matrix**"""

# Correlation Matrix
plt.figure(figsize=(10, 8))
corr_matrix = df[numerical_cols].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Numerical Variables')
plt.show()

"""## **Transaction Amount by Transaction Type**"""

# Transaction Amount by Transaction Type
plt.figure(figsize=(8, 6))
sns.boxplot(x='TransactionType', y='TransactionAmount', data=df)
plt.title('Transaction Amount by Transaction Type')
plt.xlabel('Transaction Type')
plt.ylabel('Transaction Amount')
plt.show()

"""## **Number of Transactions by Hour of Day**"""

# Number of Transactions by Hour of Day
plt.figure(figsize=(12, 6))
sns.countplot(x='TransactionHour', data=df, palette='viridis')
plt.title('Number of Transactions by Hour of Day')
plt.xlabel('Hour of Day')
plt.ylabel('Number of Transactions')
plt.show()

"""## **Number of Transactions by Day of Week**"""

# Number of Transactions by Day of Week
plt.figure(figsize=(10, 5))
sns.countplot(x='TransactionDay', data=df, palette='Set2')
plt.title('Number of Transactions by Day of Week')
plt.xlabel('Day of Week (0=Monday)')
plt.ylabel('Number of Transactions')
plt.show()

"""## **Login Attempts vs Transaction Amount**"""

# Login Attempts vs Transaction Amount
plt.figure(figsize=(10, 6))
sns.scatterplot(x='LoginAttempts', y='TransactionAmount', hue='TransactionType', data=df)
plt.title('Login Attempts vs Transaction Amount')
plt.xlabel('Login Attempts')
plt.ylabel('Transaction Amount')
plt.legend(title='Transaction Type')
plt.show()

"""## **Customer Age Distribution by Occupation**"""

# Customer Age Distribution by Occupation
plt.figure(figsize=(12, 6))
sns.boxplot(x='CustomerOccupation', y='CustomerAge', data=df)
plt.title('Customer Age by Occupation')
plt.xlabel('Customer Occupation')
plt.ylabel('Customer Age')
plt.xticks(rotation=45)
plt.show()

"""## **Transaction Duration by Channel**"""

# Transaction Duration by Channel
plt.figure(figsize=(10, 6))
sns.boxplot(x='Channel', y='TransactionDuration', data=df)
plt.title('Transaction Duration by Channel')
plt.xlabel('Channel')
plt.ylabel('Transaction Duration (seconds)')
plt.xticks(rotation=45)
plt.show()

"""# **Feature Engineering**"""

# Encoding Categorical Variables using Label Encoding
label_encoder = LabelEncoder()
# df['TransactionType_Enc'] = label_encoder.fit_transform(df['TransactionType'])
# df['Channel_Enc'] = label_encoder.fit_transform(df['Channel'])
# df['CustomerOccupation_Enc'] = label_encoder.fit_transform(df['CustomerOccupation'])

# Calculating Time Since Last Transaction
# df['TimeSinceLastTransaction'] = (df['TransactionDate'] - df['PreviousTransactionDate']).dt.total_seconds()
# df['TimeSinceLastTransaction'] = df['TimeSinceLastTransaction'].fillna(df['TimeSinceLastTransaction'].median())

# Creating Transaction Frequency Feature
transaction_counts = df.groupby('AccountID').size().reset_index(name='TransactionCount')
df = pd.merge(df, transaction_counts, on='AccountID', how='left')

# Dropping unnecessary columns for modeling
df_model = df.drop(['TransactionID', 'AccountID', 'TransactionDate', 'TransactionType', 'Location', 'DeviceID',
                    'IP Address', 'MerchantID', 'Channel', 'PreviousTransactionDate', 'CustomerOccupation'], axis=1)

"""# **Anomaly Detection using Isolation Forest**"""

# Preparing data for anomaly detection
features = df_model.columns.drop(['Anomaly']) if 'Anomaly' in df_model.columns else df_model.columns
X = df_model[features]

# Initializing and fitting the Isolation Forest model
isolation_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)
isolation_forest.fit(X)

# Predicting anomalies
df['Anomaly'] = isolation_forest.predict(X)
df['Anomaly'] = df['Anomaly'].map({1: 0, -1: 1})

# Number of anomalies detected
print(f"Number of anomalies detected: {df['Anomaly'].sum()}")

# Visualizing anomalies
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='TransactionAmount', y='AccountBalance', hue='Anomaly', palette={0: 'blue', 1: 'red'})
plt.title('Anomaly Detection using Isolation Forest')
plt.xlabel('Transaction Amount')
plt.ylabel('Account Balance')
plt.legend(title='Anomaly', labels=['Normal', 'Anomaly'])
plt.show()

"""# **Predictive Modeling**"""

# Using anomalies detected by Isolation Forest as labels
X = df_model[features]
y = df['Anomaly']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Training a Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Making predictions
y_pred = rf_classifier.predict(X_test)

# Evaluating the model
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""# **Model Improvement and Hyperparameter Tuning**"""

# Hyperparameter tuning using GridSearchCV (optional)
from sklearn.model_selection import GridSearchCV

# Defining parameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'max_features': ['auto', 'sqrt']
}

# Initializing GridSearchCV
grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),
                           param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)

# Fitting the model
grid_search.fit(X_train, y_train)

# Best parameters
print(f"Best parameters found: {grid_search.best_params_}")

# Evaluating the best estimator
best_rf = grid_search.best_estimator_
y_pred_best = best_rf.predict(X_test)
print("Classification Report with Best Estimator:")
print(classification_report(y_test, y_pred_best))

df.to_csv('prcessed_bank_transactions.csv', index=False)

import pickle
#from tensorflow import keras # No need to import keras for saving a RandomForestClassifier

# Save the model to a pickle file # Only pickle is suitable for RandomForestClassifier
with open('fraud_detection_model.pkl', 'wb') as file:
    pickle.dump(best_rf, file)
# fraud_detection_model.feature_names_in_

# prompt: load the model in pkl file and find the order of the feature name

import pickle

# Load the model from the pickle file
with open('fraud_detection_model.pkl', 'rb') as file:
    fraud_detection_model = pickle.load(file)

# Get the order of feature names
feature_names = fraud_detection_model.feature_names_in_

# Print the feature names
feature_names

